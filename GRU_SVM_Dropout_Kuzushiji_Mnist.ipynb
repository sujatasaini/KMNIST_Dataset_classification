{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU-SVM_Dropout_Kuzushiji-Mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujatasaini/KMNIST_Dataset_classification/blob/master/GRU_SVM_Dropout_Kuzushiji_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MuTQHCZ_xwCq",
        "colab_type": "code",
        "outputId": "99440e5d-6046-4d58-ec2e-ad89a016c76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0xE7iV0bxypj",
        "colab_type": "code",
        "outputId": "201d2bdf-556d-4824-bc83-886ca9ef985b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, total, unit: x  # If tqdm doesn't exist, replace it with a function that does nothing\n",
        "    print('**** Could not import tqdm. Please install tqdm for download progressbars! (pip install tqdm) ****')\n",
        "\n",
        "# Python2 compatibility\n",
        "try:\n",
        "    input = raw_input\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "download_dict = {\n",
        "    '1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)': {\n",
        "        '1) MNIST data format (ubyte.gz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'],\n",
        "        '2) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'],\n",
        "    },\n",
        "    '2) Kuzushiji-49 (49 classes, 28x28, 270k examples)': {\n",
        "        '1) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz'],\n",
        "    },\n",
        "    '3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)': {\n",
        "        '1) Folders of images (.tar)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar'],\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "# Download a list of files\n",
        "def download_list(url_list):\n",
        "    for url in url_list:\n",
        "        path = url.split('/')[-1]\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    print('All dataset files downloaded!')\n",
        "\n",
        "# Ask the user about which path to take down the dict\n",
        "def traverse_dict(d):\n",
        "    print('Please select a download option:')\n",
        "    keys = sorted(d.keys())  # Print download options\n",
        "    for key in keys:\n",
        "        print(key)\n",
        "\n",
        "    userinput = input('> ').strip()\n",
        "\n",
        "    try:\n",
        "        selection = int(userinput) - 1\n",
        "    except ValueError:\n",
        "        print('Your selection was not valid')\n",
        "        traverse_dict(d)  # Try again if input was not valid\n",
        "        return\n",
        "\n",
        "    selected = keys[selection]\n",
        "\n",
        "    next_level = d[selected]\n",
        "    if isinstance(next_level, list):  # If we've hit a list of downloads, download that list\n",
        "        download_list(next_level)\n",
        "    else:\n",
        "        traverse_dict(next_level)     # Otherwise, repeat with the next level\n",
        "\n",
        "traverse_dict(download_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please select a download option:\n",
            "1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)\n",
            "2) Kuzushiji-49 (49 classes, 28x28, 270k examples)\n",
            "3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)\n",
            "> 1\n",
            "Please select a download option:\n",
            "1) MNIST data format (ubyte.gz)\n",
            "2) NumPy data format (.npz)\n",
            "> 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 14/17740 [00:00<02:34, 114.88KB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz - 17.7 MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17740/17740 [00:07<00:00, 2301.13KB/s]\n",
            "100%|██████████| 29/29 [00:00<00:00, 223.72KB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading train-labels-idx1-ubyte.gz - 0.0 MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 14/2970 [00:00<00:24, 118.54KB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading t10k-images-idx3-ubyte.gz - 3.0 MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2970/2970 [00:02<00:00, 1007.41KB/s]\n",
            " 83%|████████▎ | 5/6 [00:00<00:00, 2324.23KB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading t10k-labels-idx1-ubyte.gz - 0.0 MB\n",
            "All dataset files downloaded!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fRs3luasx5a-",
        "colab_type": "code",
        "outputId": "87705041-57ad-45c0-8521-5e1b4570cd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Implementation of GRU+SVM model for MNIST with Dropout\"\"\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "  \n",
        "data = input_data.read_data_sets('kmnist/kmnist/dataset/kmnist/data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-90cb8b33c610>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting kmnist/kmnist/dataset/kmnist/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting kmnist/kmnist/dataset/kmnist/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting kmnist/kmnist/dataset/kmnist/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting kmnist/kmnist/dataset/kmnist/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dy4MHM1-x6Qc",
        "colab_type": "code",
        "outputId": "b6cba816-e79d-4bbe-8822-afbdc3b951fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2137
        }
      },
      "cell_type": "code",
      "source": [
        "# hyper-parameters\n",
        "BATCH_SIZE = 256\n",
        "CELL_SIZE = 256\n",
        "DROPOUT_P_KEEP = 0.85\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_CLASSES = 10\n",
        "SVM_C = 1\n",
        "\n",
        "# dataset dimension\n",
        "CHUNK_SIZE = 28\n",
        "NUM_CHUNKS = 28\n",
        "\n",
        "CHECKPOINT_PATH = 'checkpoint/'\n",
        "MODEL_NAME = 'model.ckpt'\n",
        "\n",
        "LOGS_PATH = 'logs/rnn/'\n",
        "\n",
        "x = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CHUNKS, CHUNK_SIZE], name='x_input')\n",
        "y = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CLASSES], name='y_input')\n",
        "h = tf.placeholder(dtype=tf.float32, shape=[None, CELL_SIZE], name='state')\n",
        "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
        "p_keep = tf.placeholder(dtype=tf.float32, name='p_keep')\n",
        "\n",
        "\n",
        "def variable_summaries(var):\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)\n",
        "\n",
        "\n",
        "def recurrent_neural_network(x):\n",
        "    with tf.name_scope('weights_and_biases'):\n",
        "        with tf.name_scope('weights'):\n",
        "            xav_init = tf.contrib.layers.xavier_initializer\n",
        "            weight = tf.get_variable('weights', shape=[CELL_SIZE, NUM_CLASSES], initializer=xav_init())\n",
        "            variable_summaries(weight)\n",
        "        with tf.name_scope('biases'):\n",
        "            bias = tf.get_variable('biases', initializer=tf.constant(0.1, shape=[NUM_CLASSES]))\n",
        "            variable_summaries(bias)\n",
        "\n",
        "    cell = tf.contrib.rnn.GRUCell(CELL_SIZE)\n",
        "    drop_cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=p_keep)\n",
        "    \n",
        "    outputs, states = tf.nn.dynamic_rnn(drop_cell, x, initial_state=h, dtype=tf.float32)\n",
        "\n",
        "    states = tf.identity(states, name='H')\n",
        "    hf = tf.transpose(outputs, [1, 0, 2])\n",
        "    last = tf.gather(hf, int(hf.get_shape()[0]) - 1)\n",
        "    with tf.name_scope('Wx_plus_b'):\n",
        "        output = tf.matmul(last, weight) + bias\n",
        "        tf.summary.histogram('pre-activations', output)\n",
        "\n",
        "    return output, weight, states\n",
        "\n",
        "\n",
        "def train_neural_network(x):\n",
        "    prediction, weight, states = recurrent_neural_network(x)\n",
        "\n",
        "    with tf.name_scope('loss'):\n",
        "        regularization_loss = 0.5 * tf.reduce_sum(tf.square(weight))\n",
        "        hinge_loss = tf.reduce_sum(tf.square(tf.maximum(tf.zeros([BATCH_SIZE, NUM_CLASSES]),\n",
        "                                                        1 - tf.cast(y, tf.float32) * prediction)))\n",
        "        with tf.name_scope('loss'):\n",
        "            cost = regularization_loss + SVM_C * hinge_loss\n",
        "    tf.summary.scalar('loss', cost)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "    with tf.name_scope('accuracy'):\n",
        "        predicted_class = tf.sign(prediction)\n",
        "        predicted_class = tf.identity(predicted_class, name='prediction')\n",
        "        with tf.name_scope('correct_prediction'):\n",
        "            correct = tf.equal(tf.argmax(predicted_class, 1), tf.argmax(y, 1))\n",
        "        with tf.name_scope('accuracy'):\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    timestamp = str(time.asctime())\n",
        "    writer = tf.summary.FileWriter(LOGS_PATH + timestamp, graph=tf.get_default_graph())\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "\n",
        "    current_state = np.zeros([BATCH_SIZE, CELL_SIZE])\n",
        "\n",
        "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "\n",
        "        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\n",
        "\n",
        "        if checkpoint and checkpoint.model_checkpoint_path:\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "        try:\n",
        "            for epoch in range(EPOCHS):\n",
        "                epoch_loss = 0\n",
        "                for _ in range(int(data.train.num_examples / BATCH_SIZE)):\n",
        "                    epoch_x, epoch_y = data.train.next_batch(BATCH_SIZE)\n",
        "                    epoch_y[epoch_y == 0] = -1\n",
        "\n",
        "                    epoch_x = epoch_x.reshape((BATCH_SIZE, NUM_CHUNKS, CHUNK_SIZE))\n",
        "\n",
        "                    feed_dict = {x: epoch_x, y: epoch_y, h: current_state,\n",
        "                                 learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}\n",
        "\n",
        "                    summary, _, next_state, c, accuracy_ = sess.run([merged, optimizer, states, cost, accuracy],\n",
        "                                                                    feed_dict=feed_dict)\n",
        "\n",
        "                    epoch_loss = c\n",
        "                    current_state = next_state\n",
        "\n",
        "                if epoch % 2 == 0:\n",
        "                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=epoch)\n",
        "                writer.add_summary(summary, epoch)\n",
        "                print('Epoch : {} completed out of {}, loss : {}, accuracy : {}'.format(epoch, EPOCHS,\n",
        "                                                                                        epoch_loss, accuracy_))\n",
        "        except KeyboardInterrupt:\n",
        "            print('Training interrupted at {}'.format(epoch))\n",
        "        finally:\n",
        "            writer.close()\n",
        "\n",
        "        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=epoch)\n",
        "\n",
        "        x_ = data.test.images.reshape((-1, NUM_CHUNKS, CHUNK_SIZE))\n",
        "        y_ = data.test.labels\n",
        "        y_[y_ == 0] = -1\n",
        "\n",
        "        accuracy_ = sess.run(accuracy, feed_dict={x: x_, y: y_,\n",
        "                                                  h: np.zeros([10000, CELL_SIZE]),\n",
        "                                                  p_keep: 1.0})\n",
        "\n",
        "        print('Accuracy : {}'.format(accuracy_))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_neural_network(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-4-90550eb691d3>:47: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-90550eb691d3>:50: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/model.ckpt-2\n",
            "Epoch : 0 completed out of 100, loss : 18.976234436035156, accuracy : 0.98828125\n",
            "Epoch : 1 completed out of 100, loss : 26.398448944091797, accuracy : 0.984375\n",
            "Epoch : 2 completed out of 100, loss : 9.485129356384277, accuracy : 0.99609375\n",
            "Epoch : 3 completed out of 100, loss : 24.194019317626953, accuracy : 0.9921875\n",
            "Epoch : 4 completed out of 100, loss : 16.69883155822754, accuracy : 0.98828125\n",
            "Epoch : 5 completed out of 100, loss : 20.391862869262695, accuracy : 0.9921875\n",
            "Epoch : 6 completed out of 100, loss : 15.472428321838379, accuracy : 0.9921875\n",
            "Epoch : 7 completed out of 100, loss : 6.145691871643066, accuracy : 0.99609375\n",
            "Epoch : 8 completed out of 100, loss : 17.344924926757812, accuracy : 0.984375\n",
            "Epoch : 9 completed out of 100, loss : 22.249752044677734, accuracy : 0.98046875\n",
            "Epoch : 10 completed out of 100, loss : 16.36968421936035, accuracy : 0.9921875\n",
            "Epoch : 11 completed out of 100, loss : 13.248045921325684, accuracy : 0.9921875\n",
            "Epoch : 12 completed out of 100, loss : 18.993284225463867, accuracy : 0.98828125\n",
            "Epoch : 13 completed out of 100, loss : 30.686569213867188, accuracy : 0.984375\n",
            "Epoch : 14 completed out of 100, loss : 3.857062816619873, accuracy : 1.0\n",
            "Epoch : 15 completed out of 100, loss : 4.235623359680176, accuracy : 0.99609375\n",
            "Epoch : 16 completed out of 100, loss : 11.70092487335205, accuracy : 0.99609375\n",
            "Epoch : 17 completed out of 100, loss : 6.324103832244873, accuracy : 0.99609375\n",
            "Epoch : 18 completed out of 100, loss : 6.275634288787842, accuracy : 0.99609375\n",
            "Epoch : 19 completed out of 100, loss : 9.528002738952637, accuracy : 0.99609375\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Epoch : 20 completed out of 100, loss : 12.18122386932373, accuracy : 0.98828125\n",
            "Epoch : 21 completed out of 100, loss : 12.301599502563477, accuracy : 0.9921875\n",
            "Epoch : 22 completed out of 100, loss : 3.9031903743743896, accuracy : 1.0\n",
            "Epoch : 23 completed out of 100, loss : 1.7569916248321533, accuracy : 1.0\n",
            "Epoch : 24 completed out of 100, loss : 16.315950393676758, accuracy : 0.9921875\n",
            "Epoch : 25 completed out of 100, loss : 2.1414737701416016, accuracy : 1.0\n",
            "Epoch : 26 completed out of 100, loss : 8.261740684509277, accuracy : 0.9921875\n",
            "Epoch : 27 completed out of 100, loss : 8.729372024536133, accuracy : 0.9921875\n",
            "Epoch : 28 completed out of 100, loss : 1.7948271036148071, accuracy : 1.0\n",
            "Epoch : 29 completed out of 100, loss : 2.2130136489868164, accuracy : 1.0\n",
            "Epoch : 30 completed out of 100, loss : 2.3510403633117676, accuracy : 1.0\n",
            "Epoch : 31 completed out of 100, loss : 5.06437349319458, accuracy : 0.99609375\n",
            "Epoch : 32 completed out of 100, loss : 11.699003219604492, accuracy : 0.984375\n",
            "Epoch : 33 completed out of 100, loss : 4.989405155181885, accuracy : 0.99609375\n",
            "Epoch : 34 completed out of 100, loss : 7.982938766479492, accuracy : 0.99609375\n",
            "Epoch : 35 completed out of 100, loss : 5.180770397186279, accuracy : 1.0\n",
            "Epoch : 36 completed out of 100, loss : 5.311101913452148, accuracy : 1.0\n",
            "Epoch : 37 completed out of 100, loss : 16.098289489746094, accuracy : 0.9921875\n",
            "Epoch : 38 completed out of 100, loss : 9.244150161743164, accuracy : 0.99609375\n",
            "Epoch : 39 completed out of 100, loss : 4.110804080963135, accuracy : 0.99609375\n",
            "Epoch : 40 completed out of 100, loss : 1.3662264347076416, accuracy : 1.0\n",
            "Epoch : 41 completed out of 100, loss : 3.5801875591278076, accuracy : 1.0\n",
            "Epoch : 42 completed out of 100, loss : 13.103315353393555, accuracy : 0.99609375\n",
            "Epoch : 43 completed out of 100, loss : 15.765396118164062, accuracy : 0.98828125\n",
            "Epoch : 44 completed out of 100, loss : 1.4344992637634277, accuracy : 1.0\n",
            "Epoch : 45 completed out of 100, loss : 17.385786056518555, accuracy : 0.9921875\n",
            "Epoch : 46 completed out of 100, loss : 7.331851005554199, accuracy : 0.9921875\n",
            "Epoch : 47 completed out of 100, loss : 1.5756971836090088, accuracy : 1.0\n",
            "Epoch : 48 completed out of 100, loss : 1.2688546180725098, accuracy : 1.0\n",
            "Epoch : 49 completed out of 100, loss : 1.5778722763061523, accuracy : 1.0\n",
            "Epoch : 50 completed out of 100, loss : 2.511289596557617, accuracy : 1.0\n",
            "Epoch : 51 completed out of 100, loss : 9.26862621307373, accuracy : 0.99609375\n",
            "Epoch : 52 completed out of 100, loss : 10.601500511169434, accuracy : 0.9921875\n",
            "Epoch : 53 completed out of 100, loss : 2.79844331741333, accuracy : 1.0\n",
            "Epoch : 54 completed out of 100, loss : 7.306776523590088, accuracy : 0.99609375\n",
            "Epoch : 55 completed out of 100, loss : 2.432476282119751, accuracy : 1.0\n",
            "Epoch : 56 completed out of 100, loss : 3.083714008331299, accuracy : 1.0\n",
            "Epoch : 57 completed out of 100, loss : 1.1995618343353271, accuracy : 1.0\n",
            "Epoch : 58 completed out of 100, loss : 6.727802276611328, accuracy : 0.99609375\n",
            "Epoch : 59 completed out of 100, loss : 6.716582298278809, accuracy : 0.9921875\n",
            "Epoch : 60 completed out of 100, loss : 2.13008189201355, accuracy : 1.0\n",
            "Epoch : 61 completed out of 100, loss : 4.845536708831787, accuracy : 0.99609375\n",
            "Epoch : 62 completed out of 100, loss : 8.892854690551758, accuracy : 0.9921875\n",
            "Epoch : 63 completed out of 100, loss : 8.200769424438477, accuracy : 0.99609375\n",
            "Epoch : 64 completed out of 100, loss : 5.970173358917236, accuracy : 0.99609375\n",
            "Epoch : 65 completed out of 100, loss : 2.0958127975463867, accuracy : 1.0\n",
            "Epoch : 66 completed out of 100, loss : 2.2983691692352295, accuracy : 1.0\n",
            "Epoch : 67 completed out of 100, loss : 1.0827357769012451, accuracy : 1.0\n",
            "Epoch : 68 completed out of 100, loss : 5.448211193084717, accuracy : 0.99609375\n",
            "Epoch : 69 completed out of 100, loss : 1.0512875318527222, accuracy : 1.0\n",
            "Epoch : 70 completed out of 100, loss : 5.2209296226501465, accuracy : 0.99609375\n",
            "Epoch : 71 completed out of 100, loss : 12.906723976135254, accuracy : 0.99609375\n",
            "Epoch : 72 completed out of 100, loss : 2.805734634399414, accuracy : 1.0\n",
            "Epoch : 73 completed out of 100, loss : 0.8912333846092224, accuracy : 1.0\n",
            "Epoch : 74 completed out of 100, loss : 14.380702018737793, accuracy : 0.99609375\n",
            "Epoch : 75 completed out of 100, loss : 1.283229947090149, accuracy : 1.0\n",
            "Epoch : 76 completed out of 100, loss : 1.4147891998291016, accuracy : 1.0\n",
            "Epoch : 77 completed out of 100, loss : 1.724745273590088, accuracy : 1.0\n",
            "Epoch : 78 completed out of 100, loss : 3.2094407081604004, accuracy : 1.0\n",
            "Epoch : 79 completed out of 100, loss : 1.292853593826294, accuracy : 1.0\n",
            "Epoch : 80 completed out of 100, loss : 2.185690402984619, accuracy : 1.0\n",
            "Epoch : 81 completed out of 100, loss : 22.559228897094727, accuracy : 0.98828125\n",
            "Epoch : 82 completed out of 100, loss : 7.806647300720215, accuracy : 0.9921875\n",
            "Epoch : 83 completed out of 100, loss : 10.99499797821045, accuracy : 0.99609375\n",
            "Epoch : 84 completed out of 100, loss : 1.2635092735290527, accuracy : 1.0\n",
            "Epoch : 85 completed out of 100, loss : 1.691353440284729, accuracy : 1.0\n",
            "Epoch : 86 completed out of 100, loss : 2.7203257083892822, accuracy : 1.0\n",
            "Epoch : 87 completed out of 100, loss : 5.779189109802246, accuracy : 0.99609375\n",
            "Epoch : 88 completed out of 100, loss : 2.6715080738067627, accuracy : 1.0\n",
            "Epoch : 89 completed out of 100, loss : 8.460041999816895, accuracy : 0.99609375\n",
            "Epoch : 90 completed out of 100, loss : 7.655777454376221, accuracy : 0.99609375\n",
            "Epoch : 91 completed out of 100, loss : 4.737929821014404, accuracy : 0.99609375\n",
            "Epoch : 92 completed out of 100, loss : 2.436082363128662, accuracy : 1.0\n",
            "Epoch : 93 completed out of 100, loss : 1.743546485900879, accuracy : 1.0\n",
            "Epoch : 94 completed out of 100, loss : 2.3261964321136475, accuracy : 1.0\n",
            "Epoch : 95 completed out of 100, loss : 1.1396095752716064, accuracy : 1.0\n",
            "Epoch : 96 completed out of 100, loss : 5.082447528839111, accuracy : 0.99609375\n",
            "Epoch : 97 completed out of 100, loss : 17.904966354370117, accuracy : 0.9921875\n",
            "Epoch : 98 completed out of 100, loss : 5.429013252258301, accuracy : 0.99609375\n",
            "Epoch : 99 completed out of 100, loss : 5.998327732086182, accuracy : 0.99609375\n",
            "Accuracy : 0.9900000095367432\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}