{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU-SVM_Dropout_Fashion-Mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujatasaini/KMNIST_Dataset_classification/blob/master/GRU_SVM_Dropout_Fashion_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zkGqpSfLfp3m",
        "colab_type": "code",
        "outputId": "672dfc98-16e8-468c-fa07-c2fd410fccf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SrcXxzYff6KG",
        "colab_type": "code",
        "outputId": "9658df40-a9af-4d34-c757-82f99616025f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2557
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Implementation of GRU+SVM model for Fashion MNIST with Dropout\"\"\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "data = input_data.read_data_sets('/home/darth/GitHub Projects/fashion-mnist/data/fashion', one_hot=True)\n",
        "\n",
        "# hyper-parameters\n",
        "BATCH_SIZE = 256\n",
        "CELL_SIZE = 256\n",
        "DROPOUT_P_KEEP = 0.85\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_CLASSES = 10\n",
        "SVM_C = 1\n",
        "\n",
        "# dataset dimension\n",
        "CHUNK_SIZE = 28\n",
        "NUM_CHUNKS = 28\n",
        "\n",
        "CHECKPOINT_PATH = 'checkpoint/'\n",
        "MODEL_NAME = 'model.ckpt'\n",
        "\n",
        "LOGS_PATH = 'logs/rnn/'\n",
        "\n",
        "x = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CHUNKS, CHUNK_SIZE], name='x_input')\n",
        "y = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CLASSES], name='y_input')\n",
        "h = tf.placeholder(dtype=tf.float32, shape=[None, CELL_SIZE], name='state')\n",
        "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
        "p_keep = tf.placeholder(dtype=tf.float32, name='p_keep')\n",
        "\n",
        "\n",
        "def variable_summaries(var):\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)\n",
        "\n",
        "\n",
        "def recurrent_neural_network(x):\n",
        "    with tf.name_scope('weights_and_biases'):\n",
        "        with tf.name_scope('weights'):\n",
        "            xav_init = tf.contrib.layers.xavier_initializer\n",
        "            weight = tf.get_variable('weights', shape=[CELL_SIZE, NUM_CLASSES], initializer=xav_init())\n",
        "            variable_summaries(weight)\n",
        "        with tf.name_scope('biases'):\n",
        "            bias = tf.get_variable('biases', initializer=tf.constant(0.1, shape=[NUM_CLASSES]))\n",
        "            variable_summaries(bias)\n",
        "\n",
        "    cell = tf.contrib.rnn.GRUCell(CELL_SIZE)\n",
        "    drop_cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=p_keep)\n",
        "    \n",
        "    outputs, states = tf.nn.dynamic_rnn(drop_cell, x, initial_state=h, dtype=tf.float32)\n",
        "\n",
        "    states = tf.identity(states, name='H')\n",
        "    hf = tf.transpose(outputs, [1, 0, 2])\n",
        "    last = tf.gather(hf, int(hf.get_shape()[0]) - 1)\n",
        "    with tf.name_scope('Wx_plus_b'):\n",
        "        output = tf.matmul(last, weight) + bias\n",
        "        tf.summary.histogram('pre-activations', output)\n",
        "\n",
        "    return output, weight, states\n",
        "\n",
        "\n",
        "def train_neural_network(x):\n",
        "    prediction, weight, states = recurrent_neural_network(x)\n",
        "\n",
        "    with tf.name_scope('loss'):\n",
        "        regularization_loss = 0.5 * tf.reduce_sum(tf.square(weight))\n",
        "        hinge_loss = tf.reduce_sum(tf.square(tf.maximum(tf.zeros([BATCH_SIZE, NUM_CLASSES]),\n",
        "                                                        1 - tf.cast(y, tf.float32) * prediction)))\n",
        "        with tf.name_scope('loss'):\n",
        "            cost = regularization_loss + SVM_C * hinge_loss\n",
        "    tf.summary.scalar('loss', cost)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "    with tf.name_scope('accuracy'):\n",
        "        predicted_class = tf.sign(prediction)\n",
        "        predicted_class = tf.identity(predicted_class, name='prediction')\n",
        "        with tf.name_scope('correct_prediction'):\n",
        "            correct = tf.equal(tf.argmax(predicted_class, 1), tf.argmax(y, 1))\n",
        "        with tf.name_scope('accuracy'):\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    timestamp = str(time.asctime())\n",
        "    writer = tf.summary.FileWriter(LOGS_PATH + timestamp, graph=tf.get_default_graph())\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "\n",
        "    current_state = np.zeros([BATCH_SIZE, CELL_SIZE])\n",
        "\n",
        "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "\n",
        "        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\n",
        "\n",
        "        if checkpoint and checkpoint.model_checkpoint_path:\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "        try:\n",
        "            for epoch in range(EPOCHS):\n",
        "                epoch_loss = 0\n",
        "                for _ in range(int(data.train.num_examples / BATCH_SIZE)):\n",
        "                    epoch_x, epoch_y = data.train.next_batch(BATCH_SIZE)\n",
        "                    epoch_y[epoch_y == 0] = -1\n",
        "\n",
        "                    epoch_x = epoch_x.reshape((BATCH_SIZE, NUM_CHUNKS, CHUNK_SIZE))\n",
        "\n",
        "                    feed_dict = {x: epoch_x, y: epoch_y, h: current_state,\n",
        "                                 learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}\n",
        "\n",
        "                    summary, _, next_state, c, accuracy_ = sess.run([merged, optimizer, states, cost, accuracy],\n",
        "                                                                    feed_dict=feed_dict)\n",
        "\n",
        "                    epoch_loss = c\n",
        "                    current_state = next_state\n",
        "\n",
        "                if epoch % 2 == 0:\n",
        "                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=epoch)\n",
        "                writer.add_summary(summary, epoch)\n",
        "                print('Epoch : {} completed out of {}, loss : {}, accuracy : {}'.format(epoch, EPOCHS,\n",
        "                                                                                        epoch_loss, accuracy_))\n",
        "        except KeyboardInterrupt:\n",
        "            print('Training interrupted at {}'.format(epoch))\n",
        "        finally:\n",
        "            writer.close()\n",
        "\n",
        "        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=epoch)\n",
        "\n",
        "        x_ = data.test.images.reshape((-1, NUM_CHUNKS, CHUNK_SIZE))\n",
        "        y_ = data.test.labels\n",
        "        y_[y_ == 0] = -1\n",
        "\n",
        "        accuracy_ = sess.run(accuracy, feed_dict={x: x_, y: y_,\n",
        "                                                  h: np.zeros([10000, CELL_SIZE]),\n",
        "                                                  p_keep: 1.0})\n",
        "\n",
        "        print('Accuracy : {}'.format(accuracy_))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_neural_network(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-49df0e21864c>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /home/darth/GitHub Projects/fashion-mnist/data/fashion/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /home/darth/GitHub Projects/fashion-mnist/data/fashion/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /home/darth/GitHub Projects/fashion-mnist/data/fashion/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /home/darth/GitHub Projects/fashion-mnist/data/fashion/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-2-49df0e21864c>:56: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-49df0e21864c>:59: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 completed out of 100, loss : 156.84075927734375, accuracy : 0.8828125\n",
            "Epoch : 1 completed out of 100, loss : 77.72896575927734, accuracy : 0.9453125\n",
            "Epoch : 2 completed out of 100, loss : 53.193050384521484, accuracy : 0.9609375\n",
            "Epoch : 3 completed out of 100, loss : 41.543304443359375, accuracy : 0.97265625\n",
            "Epoch : 4 completed out of 100, loss : 47.721900939941406, accuracy : 0.96875\n",
            "Epoch : 5 completed out of 100, loss : 24.809856414794922, accuracy : 0.9921875\n",
            "Epoch : 6 completed out of 100, loss : 38.9558219909668, accuracy : 0.98046875\n",
            "Epoch : 7 completed out of 100, loss : 31.449647903442383, accuracy : 0.98046875\n",
            "Epoch : 8 completed out of 100, loss : 32.18021011352539, accuracy : 0.98828125\n",
            "Epoch : 9 completed out of 100, loss : 16.407991409301758, accuracy : 0.9921875\n",
            "Epoch : 10 completed out of 100, loss : 33.143280029296875, accuracy : 0.98046875\n",
            "Epoch : 11 completed out of 100, loss : 14.738639831542969, accuracy : 0.9921875\n",
            "Epoch : 12 completed out of 100, loss : 21.498428344726562, accuracy : 0.98046875\n",
            "Epoch : 13 completed out of 100, loss : 18.734312057495117, accuracy : 0.9921875\n",
            "Epoch : 14 completed out of 100, loss : 10.867643356323242, accuracy : 0.99609375\n",
            "Epoch : 15 completed out of 100, loss : 17.478017807006836, accuracy : 0.984375\n",
            "Epoch : 16 completed out of 100, loss : 14.148073196411133, accuracy : 0.9921875\n",
            "Epoch : 17 completed out of 100, loss : 8.709053993225098, accuracy : 0.99609375\n",
            "Epoch : 18 completed out of 100, loss : 23.648996353149414, accuracy : 0.98828125\n",
            "Epoch : 19 completed out of 100, loss : 7.690585136413574, accuracy : 0.99609375\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Epoch : 20 completed out of 100, loss : 5.732295513153076, accuracy : 1.0\n",
            "Epoch : 21 completed out of 100, loss : 7.038254737854004, accuracy : 0.99609375\n",
            "Epoch : 22 completed out of 100, loss : 6.63361120223999, accuracy : 0.99609375\n",
            "Epoch : 23 completed out of 100, loss : 6.034441947937012, accuracy : 1.0\n",
            "Epoch : 24 completed out of 100, loss : 5.954575538635254, accuracy : 1.0\n",
            "Epoch : 25 completed out of 100, loss : 6.5867438316345215, accuracy : 1.0\n",
            "Epoch : 26 completed out of 100, loss : 7.195364952087402, accuracy : 0.99609375\n",
            "Epoch : 27 completed out of 100, loss : 13.539851188659668, accuracy : 0.98828125\n",
            "Epoch : 28 completed out of 100, loss : 5.83554220199585, accuracy : 0.99609375\n",
            "Epoch : 29 completed out of 100, loss : 16.771142959594727, accuracy : 0.98828125\n",
            "Epoch : 30 completed out of 100, loss : 15.597846984863281, accuracy : 0.99609375\n",
            "Epoch : 31 completed out of 100, loss : 5.165378093719482, accuracy : 0.99609375\n",
            "Epoch : 32 completed out of 100, loss : 6.600883960723877, accuracy : 0.99609375\n",
            "Epoch : 33 completed out of 100, loss : 10.752617835998535, accuracy : 0.99609375\n",
            "Epoch : 34 completed out of 100, loss : 5.051177978515625, accuracy : 1.0\n",
            "Epoch : 35 completed out of 100, loss : 17.537403106689453, accuracy : 0.99609375\n",
            "Epoch : 36 completed out of 100, loss : 10.96609878540039, accuracy : 0.9921875\n",
            "Epoch : 37 completed out of 100, loss : 11.80760383605957, accuracy : 0.99609375\n",
            "Epoch : 38 completed out of 100, loss : 15.447894096374512, accuracy : 0.98828125\n",
            "Epoch : 39 completed out of 100, loss : 2.6442136764526367, accuracy : 1.0\n",
            "Epoch : 40 completed out of 100, loss : 12.558609008789062, accuracy : 0.9921875\n",
            "Epoch : 41 completed out of 100, loss : 8.793667793273926, accuracy : 0.99609375\n",
            "Epoch : 42 completed out of 100, loss : 10.416847229003906, accuracy : 0.9921875\n",
            "Epoch : 43 completed out of 100, loss : 14.892072677612305, accuracy : 0.98828125\n",
            "Epoch : 44 completed out of 100, loss : 6.307836532592773, accuracy : 0.99609375\n",
            "Epoch : 45 completed out of 100, loss : 1.4146082401275635, accuracy : 1.0\n",
            "Epoch : 46 completed out of 100, loss : 4.5720696449279785, accuracy : 0.99609375\n",
            "Epoch : 47 completed out of 100, loss : 12.672561645507812, accuracy : 0.99609375\n",
            "Epoch : 48 completed out of 100, loss : 1.5885356664657593, accuracy : 1.0\n",
            "Epoch : 49 completed out of 100, loss : 3.8609113693237305, accuracy : 1.0\n",
            "Epoch : 50 completed out of 100, loss : 7.618319034576416, accuracy : 0.99609375\n",
            "Epoch : 51 completed out of 100, loss : 4.916967391967773, accuracy : 1.0\n",
            "Epoch : 52 completed out of 100, loss : 5.713908672332764, accuracy : 0.99609375\n",
            "Epoch : 53 completed out of 100, loss : 1.756115436553955, accuracy : 1.0\n",
            "Epoch : 54 completed out of 100, loss : 3.9288887977600098, accuracy : 0.99609375\n",
            "Epoch : 55 completed out of 100, loss : 2.502401828765869, accuracy : 1.0\n",
            "Epoch : 56 completed out of 100, loss : 5.783166408538818, accuracy : 0.99609375\n",
            "Epoch : 57 completed out of 100, loss : 3.1215734481811523, accuracy : 1.0\n",
            "Epoch : 58 completed out of 100, loss : 3.760977029800415, accuracy : 0.99609375\n",
            "Epoch : 59 completed out of 100, loss : 7.907715797424316, accuracy : 0.99609375\n",
            "Epoch : 60 completed out of 100, loss : 1.3728084564208984, accuracy : 1.0\n",
            "Epoch : 61 completed out of 100, loss : 2.0502114295959473, accuracy : 1.0\n",
            "Epoch : 62 completed out of 100, loss : 11.78298282623291, accuracy : 0.9921875\n",
            "Epoch : 63 completed out of 100, loss : 8.809003829956055, accuracy : 0.9921875\n",
            "Epoch : 64 completed out of 100, loss : 1.8425421714782715, accuracy : 1.0\n",
            "Epoch : 65 completed out of 100, loss : 2.5746009349823, accuracy : 1.0\n",
            "Epoch : 66 completed out of 100, loss : 1.5266789197921753, accuracy : 1.0\n",
            "Epoch : 67 completed out of 100, loss : 7.2082414627075195, accuracy : 0.99609375\n",
            "Epoch : 68 completed out of 100, loss : 2.5364062786102295, accuracy : 1.0\n",
            "Epoch : 69 completed out of 100, loss : 3.131622076034546, accuracy : 1.0\n",
            "Epoch : 70 completed out of 100, loss : 2.9821925163269043, accuracy : 0.99609375\n",
            "Epoch : 71 completed out of 100, loss : 0.9634422063827515, accuracy : 1.0\n",
            "Epoch : 72 completed out of 100, loss : 5.582859039306641, accuracy : 0.99609375\n",
            "Epoch : 73 completed out of 100, loss : 0.982398509979248, accuracy : 1.0\n",
            "Epoch : 74 completed out of 100, loss : 6.137841701507568, accuracy : 0.99609375\n",
            "Epoch : 75 completed out of 100, loss : 2.0076723098754883, accuracy : 1.0\n",
            "Epoch : 76 completed out of 100, loss : 1.2526822090148926, accuracy : 1.0\n",
            "Epoch : 77 completed out of 100, loss : 4.296112060546875, accuracy : 1.0\n",
            "Epoch : 78 completed out of 100, loss : 1.3232473134994507, accuracy : 1.0\n",
            "Epoch : 79 completed out of 100, loss : 8.755623817443848, accuracy : 0.99609375\n",
            "Epoch : 80 completed out of 100, loss : 1.020385980606079, accuracy : 1.0\n",
            "Epoch : 81 completed out of 100, loss : 16.12320899963379, accuracy : 0.9921875\n",
            "Epoch : 82 completed out of 100, loss : 1.341591477394104, accuracy : 1.0\n",
            "Epoch : 83 completed out of 100, loss : 4.034479141235352, accuracy : 1.0\n",
            "Epoch : 84 completed out of 100, loss : 5.65858268737793, accuracy : 0.99609375\n",
            "Epoch : 85 completed out of 100, loss : 1.79840087890625, accuracy : 1.0\n",
            "Epoch : 86 completed out of 100, loss : 2.701660394668579, accuracy : 0.99609375\n",
            "Epoch : 87 completed out of 100, loss : 4.03383731842041, accuracy : 0.99609375\n",
            "Epoch : 88 completed out of 100, loss : 1.6062548160552979, accuracy : 1.0\n",
            "Epoch : 89 completed out of 100, loss : 6.647388458251953, accuracy : 0.99609375\n",
            "Epoch : 90 completed out of 100, loss : 16.66485595703125, accuracy : 0.9921875\n",
            "Epoch : 91 completed out of 100, loss : 1.3108055591583252, accuracy : 1.0\n",
            "Epoch : 92 completed out of 100, loss : 1.782387137413025, accuracy : 1.0\n",
            "Epoch : 93 completed out of 100, loss : 1.025433897972107, accuracy : 1.0\n",
            "Epoch : 94 completed out of 100, loss : 1.0255494117736816, accuracy : 1.0\n",
            "Epoch : 95 completed out of 100, loss : 15.534918785095215, accuracy : 0.9921875\n",
            "Epoch : 96 completed out of 100, loss : 1.085784673690796, accuracy : 1.0\n",
            "Epoch : 97 completed out of 100, loss : 15.244443893432617, accuracy : 0.98828125\n",
            "Epoch : 98 completed out of 100, loss : 1.1657668352127075, accuracy : 1.0\n",
            "Epoch : 99 completed out of 100, loss : 1.754800796508789, accuracy : 1.0\n",
            "Accuracy : 0.9908999800682068\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}